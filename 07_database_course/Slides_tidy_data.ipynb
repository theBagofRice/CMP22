{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9fba321",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Database Management\n",
    "Daniel Bultrini"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e9bace",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why would we want this?\n",
    "\n",
    "As researchers we handle a lot of data. There is almost no escape unless one escapes to pure mathematics and mathematical physics, perhaps. But even when working on mostly theoretical research, it is almost certain that at some point you will be faced with the prospect of running some numerical simulations, or even generating plots from analytical functions with some parameters. With experiments, it goes without saying that the whole laboratory is built to gather and process data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31816293",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Database management is not strictly necessary to complete even ambitious projects requiring gigabytes of data, with enough patience and coping strategies. Yet having access to tools and techniques that make trawling through experimental and numerical data in a consistent way can help in two major ways. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17500d3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### open data\n",
    "First, as time goes by, more and more journals greatly appreciate open access to both the data and code used to generate the results shown in the paper. Of course, this is not always possible depending on the nature of the research, but even then, you might be asked to turn your code or data over to a reviewer. Having a reliable and easy to share way to access this data is key to make it usable far in the future. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95f4eff",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Second, and more importantly, having your data well organized and easily accessible digitally allows you to explore and plot your dataset in ways that can uncover new relationships and interesting conclusions from work, even if your aims were completely different. Furthermore, typing everything in the style we will explore in this workshop makes it extremely easy to apply powerful techniques from simpler machine learning/regression to deep neural networks to your results, which may or may not be helpful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf918f12",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Commonly used tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032b2899",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### SQL\n",
    "\n",
    "In big data, SQL databases are king, as they allow for easily distributed storage, incremental addition and \"simple\" retrievals of arbitrary subsets of the data. Using such a system is simple from python, and allows you to have all of your results in one large, queryable setup. Even if your data gathering changes, new entries don't necessarily have to conform to the old, and the old can be updated to be compatible with the new."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152e9627",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This can happen if you, for example, begin to be able to measure new properties thanks to a new sensor or a fancy new output statement in your code. We will not focus on SQL excessively, but a simple example will be given."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a4e436",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### R\n",
    "\n",
    "This language comes with a set of tools that are geared towards analysis and is loved by statisticians everywhere, and used by data scientists on a similar level to python.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380aed52",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Python and Pandas\n",
    "\n",
    "Python is one of the most commonly used open source languages in multiple branches of science outside of performance critical codes. Yet, even in such cases, python can Â integrate with a huge host of languages and programs, indeed it is possible to call functions and other executables to perform any given operation or simulation via a *wrapper* function, or an explicit call to the program (less orthodox, but sometimes required). Prebuilt wrappers or standard recipes exist to all major languages, from C to Fortran to Julia. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1394c0c6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Commercial programs such as MATLAB and Labview even provide libraries that makes sharing data a breeze. Thorlab cameras and sensors are no exception. In the worst case, one can write from one program or function into an intermediary file which is then loaded by python - this is fine for most work, but requires direct involvement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd65017",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Pandas offers a fairly simple way to deal with organizing, processing and storing data. It can interface with everything described above and more, depending on your needs. It also comes with its own powerful set of visualization tools, as well as other software written around it that can take some of the pain from statistical analysis and plotting of your work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c882942",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Database hygiene"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b751305c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We are fortunate, in running our own experiments (of whatever type they may be) that we can start structuring our data gathering correctly from the beginning. This means that we don't need to necessarily change and alter what may be very messy databases to be suitable for quick processing. That being said, as experiments change, it will be necessary to alter our structure as time goes by, so 'data cleaning' and recasting will doubtlessly be a part of the process. I hope to show you the tools to be able to do this programmatically throughout this workshop. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb02888d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A well structured dataset is tool agnostic, which is to say that the resulting datasets are easy to transfer from program to program, irrespective of what you will choose to use in your own work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a543302",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let us think of two simple ways you might save data:\n",
    "\n",
    "**Example 1**\n",
    "\n",
    "| samp | exp_1 | exp_2 | --- |\n",
    "| ---- | ----- | ----- | --- |\n",
    "| A    | 10    | -     |     |\n",
    "| B    | 15    | 6     |     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4e54a3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Example 2**\n",
    "\n",
    "| exp | samp_A | samp_B | \n",
    "| --- | ------ | ------ |\n",
    "| 1   | 10     | 15     |\n",
    "| 2   | -      | 6      |\n",
    "| --- | ---    | ---    |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1b87f2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Both of these are adequate to some extent, but imagine that you want to add some extra information, like the date (or experimental parameters)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e8df59",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Example 1a**\n",
    "\n",
    "| sample | exp_1 | exp_2 | date    |\n",
    "| ------ | ----- | ----- | ------- |\n",
    "| A      | 10    | -     | 11/3/21 |\n",
    "| B      | 15    | 6     | 12/3/21 |\n",
    "\n",
    "We already get a problem, that a date in this case, can only be linked to a single sample and not an experiment. Of course this can be somewhat remedied by making the date parameter somewhat complicated as such:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623073cb",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "**Example 1b**\n",
    "\n",
    "| samp | exp_1 | exp_2 | date_exp1 | date_exp2 |\n",
    "| ---- | ----- | ----- | --------- | --------- |\n",
    "| A    | 10    | -     | 11/3/21   | -         |\n",
    "| B    | 15    | 6     | 12/3/21   | 13/3/21   |\n",
    "\n",
    "And of course, with the alternate table we end up with something equally convulsed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1226e2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Example 2a**\n",
    "\n",
    "| exp | samp_A | samp_B | date_SampA | dateSampB | \n",
    "| --- | ------ | ------ | ---------- | ---------- |\n",
    "| 1   | 10     | 15     | 11/3/21    | 12/3/21    |\n",
    "| 2   | -      | 6      | -          | 13/3/21    |\n",
    "\n",
    "Of course, the logical conclusion of this is that you will have a lot of named columns which are related to some other columns, but not all. This makes it much harder to programmatically wrangle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1497f2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data structure and semantics\n",
    "We usually arrange our data as a collection of *rows* and *columns* forming a table. The examples above are transposed versions of each other, and a common way to arrange experimental data. \n",
    "\n",
    "The dataset is then a collection of *values* that are numeric or qualitative, which generally represent an *observation*. In our particular example we have some variables and observations which are:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9300577",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "1. **Experiment**: we have two experiments: 1 and 2.\n",
    "2. **Sample**: We have samples A and B.\n",
    "3. **Results**: various results which are related to the intersection of the sample an experiment.\n",
    "4. **Date**: This one is a marker/metadata which can help browse and store a condition of the experiment. Of course, realistically you will store things like 'power', 'error rate', etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd47851",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Everything, including the missing value (an experiment that is yet to be made) is important to this dataset and should be retained. Of course, it may be possible to also have data that cannot ever be gathered, such as atomic distance when studying a single atom, might be unnecessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b040703a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tidy data paradigm\n",
    "This paradigm was first formalized by Hadley Wickham and attempts to make a easily processed structure. The idea is that one maps the meaning of a dataset to its structure, and attempts to maximise the ratio of variable columns to observational row. \n",
    "1. Each variable forms a column.\n",
    "2. Each observation forms a row.\n",
    "3. Each type of observational unit forms a table. \n",
    "If we apply this philosophy to the data we have we get:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1f3944",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Example 3** - 'tidy data'\n",
    "\n",
    "| experiment | Sample | date    | Result |\n",
    "| ---------- | ------ | ------- | ------ |\n",
    "| 1          | A      | 11/3/21 | 10     |\n",
    "| 1          | B      | 12/3/21 | 15     |\n",
    "| 2          | A      | -       | -      |\n",
    "| 2          | B      | 13/3/21 | 6      |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de156534",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "With this small change we are now free from the cruel constraints of excessive columns and confusing organization. Now to select subsets of our results we can simply ask \n",
    "\n",
    "'*give me the rows with sample  A and experiment 1*', \n",
    "\n",
    "rather than **example 2**, which would require a convoluted query like:\n",
    "\n",
    "'*give me column of sample A and the column date_sampA selecting the rows of the column of experiment  with the value 1*'. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d006a559",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Of course, the complexity of the latter query would only increase, as all the columns related to the sample (or experiment, in the case of the first **example 1**) that you want would need to be explicitly called for. \n",
    "\n",
    "We  suffer a little bit of inefficiency, instead of 10 entries as in **1b, 2a**, we have 16 entries, but when storing such datasets, a binary tree is usually constructed during compression that represents highly duplicated values. Multi-indexing is also possible, where you have a hierarchical setup of labels, but these only become an issue when dealing with gargantuan datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386bcc7f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Using a tidy data paradigm easily allows one to have a standard way to query and analyze data, from easily generating statistical deductions, finding trends and plotting said things. When data is not structured this way, one will need to craft bespoke strategies to extract desired values, which incurs a large overhead. This can be in either crafting specific functions to query your database or doing it manually in every case, which will make looking at your results painful, and possibly limit your exploration to what is strictly necessary or pre-planned. This invites the possibility for many errors to creep in. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585d2a4b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Structuring values like this comes with more advantages, many analysis operations require all the values of a variable, such as the average of all results of experiment 1 of sample A. Such operations are known as *aggregation*. Furthermore, the resulting 'row vectors' from queries of the dataset are particularly suited to vectorization, speeding up a lot of these operations. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ada2026",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Choosing appropriate orders variables and labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee201eb7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "#### Order\n",
    "The order of the columns and rows is rather unimportant programmatically, but clearly it is sometimes useful to look at your database in person, so some care should be taken. In **3**, for example, it may be that *date* should be the last column, since it's probably not more interesting than the result. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b207b83",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "| experiment | Sample | date    | Result |\n",
    "| ---------- | ------ | ------- | ------ |\n",
    "| 1          | A      | 11/3/21 | 10     |\n",
    "| 1          | B      | 12/3/21 | 15     |\n",
    "| 2          | A      | -       | -      |\n",
    "| 2          | B      | 13/3/21 | 6      |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92eb9d38",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Good practice dictates that *fixed values*, so called due to the fact that they are fixed by the design of the data collection, come first. An example of this from our test data is the type of experiment and sample, these are known as 'dimension' in computer science, 'subscripts on random variables' to statisticians, and 'parameters' to us. This is then followed by measured variables, and if necessary, metadata at the end. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46d87bf",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Metadata could be uncontrolled experimental conditions (i.e. time since the last calibration and the calibration parameters for a quantum computer - T1, T2 times per qubit, error rates, etc. Of course, this can be a link to a file containing the calibration data).  Generally you try to order rows and measurement columns such that the related variables are contiguous. This is not strictly necessary, but is definitely helpful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76772629",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Variables and labels\n",
    "Sometimes it's easy to figure out what is a variable and what is a measurement, but it's not always so obvious on how to differentiate them. For example, if you had **volume** and **mass**, these would clearly be two columns, but if you had **X position**, **Y position**, **Z position** would you have two columns or would you make a label column (**Position**) and a value column (**Position_Value**)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b9499f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "| experiment | X   | Y   | Z   | result |\n",
    "| ---------- | --- | --- | --- | ------ |\n",
    "| 1          | 2   | 3   | 4   | 5      | \n",
    "\n",
    "or\n",
    "\n",
    "| experiment | Position coordinate | position value | result |\n",
    "| ---------- | ------------------- | -------------- | ------ |\n",
    "| 1          | X                   | 2              | 5      |\n",
    "| 1          | Y                   | 3              | 5      |\n",
    "| 1          | Z                   | 4              | 5      | "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2501367a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Of course, in this example, it is clear that the former is best, but cases like this might come up, and the latter arrangement may be useful if for whatever reason you only want to see what happens as you vary **X**.\n",
    "\n",
    "In general you want to make it as easy as possible to describe functional relationships between variables between columns and comparisons between groups of columns. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13590a0b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cleaning up data\n",
    "There are several common 'problems' that are commonly found and that you might find yourself and their 'easy' solutions.\n",
    "\n",
    "- multiple variables are stored in one column\n",
    "- variables are both in rows and columns\n",
    "- multiple types of unrelated observations are in the same table\n",
    "- data is spread over various files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f407c39b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "| problem                            | Solution            |\n",
    "| ---------------------------------- | ------------------- |\n",
    "| Column titles are values           | melting             |\n",
    "| Multiple variables in one column   | string splitting    |\n",
    "| variables in both rows and columns | melting + recasting |\n",
    "| unrelated measurements             | filter + recreate   |\n",
    "| data spread between files          | concatinate         | "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad7861b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Examples "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8860bd74",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Column titles are values\n",
    "\n",
    "**Raw data**\n",
    "\n",
    "| row | <10 | 10-20 | >20 |\n",
    "| --- | --- | ----- | --- |\n",
    "| A   | 1   | 2     | 3   |\n",
    "| B   | 4   | 5     | 6   |\n",
    "| C   | 7   | 8     | 9   |\n",
    "\n",
    "The general technique to change this type of data is always called melting, and functions automatically generate something like this:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef12bc1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Molten data**\n",
    "\n",
    "| row | column | value |\n",
    "| --- | ------ | ----- |\n",
    "| A   | <10    | 1     |\n",
    "| B   | <10    | 2     |\n",
    "| C   | <10    | 3     |\n",
    "| A   | 10-20  | 4     |\n",
    "| B   | 10-20  | 5     |\n",
    "| C   | 10-20  | 6     |\n",
    "| A   | >20    | 7     |\n",
    "| B   | >20    | 8     |\n",
    "| C   | >20    | 9     | "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1233c00",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Of course, in this case, the raw data form is exceptionally useful if one can perform matrix transformations on it, but it is generally better to have a method to filter out a dataset and generate the matrix-form data on call when that kind of processing is required. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bf76a0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Multiple values in one entry\n",
    "\n",
    "| year | column | cases |\n",
    "| ---- | ------ | ----- |\n",
    "| 2000 | m1524  | 0     |\n",
    "| 2000 | m2534  | 1     |\n",
    "| 2000 | m3544  | 0     |\n",
    "| 2000 | m4554  | 0     |\n",
    "| 2000 | m5564  | 0     |\n",
    "| 2000 | m65    | 0     |\n",
    "| 2000 | f014   | 3     |\n",
    "\n",
    "We can split such a composite column with some clever regex or coding, but whatever the case, you will want to end up with something like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19991d79",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "| year | sex | age   | cases |\n",
    "| ---- | --- | ----- | ----- |\n",
    "| 2000 | m   | 15â24 | 0     |\n",
    "| 2000 | m   | 25â34 | 1     |\n",
    "| 2000 | m   | 35â44 | 0     |\n",
    "| 2000 | m   | 45â54 | 0     |\n",
    "| 2000 | m   | 55â64 | 0     |\n",
    "| 2000 | m   | 65+   | 0     |\n",
    "| 2000 | f   | 0-14  | 3     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78db889d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Variables in both rows and columns\n",
    "\n",
    "   | year | month | element | day1 | day2 | day3 | day4 | ... |\n",
    "   | ---- | ----- | ------- | ---- | ---- | ---- | ---- | --- |\n",
    "   | 2010 | 1     | tmax    |      |      |      |      |     |\n",
    "   | 2010 | 1     | tmin    |      |      |      |      |     |\n",
    "   | 2010 | 2     | tmax    | 27.3 | 24.1 |      |      |     |\n",
    "   | 2010 | 2     | tmin    | 14.4 | 14.4 |      |      |     |\n",
    "   | 2010 | 3     | tmax    |      |      |      | 32.1 |     |\n",
    "   | 2010 | 3     | tmin    |      |      |      | 14.  |     |\n",
    "\n",
    "\n",
    "A simple melt with a function to convert year+day+month into a date gives "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3956576b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "| date       | element | value |\n",
    "| ---------- | ------- | ----- |\n",
    "| 2010-01-30 | tmax    | 27.8  |\n",
    "| 2010-01-30 | tmin    | 14.5  |\n",
    "| 2010-02-02 | tmax    | 27.3  |\n",
    "| 2010-02-02 | tmin    | 14.4  |\n",
    "| 2010-02-03 | tmax    | 24.1  |\n",
    "| 2010-02-03 | tmin    | 14.4  | \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c58f9e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "but we can do a little better by **unstacking** or **casting**.\n",
    "\n",
    "| date       | tmax | tmin |\n",
    "| ---------- | ---- | ---- |\n",
    "| 2010-01-30 | 27.8 | 14.5 |\n",
    "| 2010-02-02 | 27.3 | 14.4 |\n",
    "| 2010-02-03 | 24.1 | 14.4 |\n",
    "| 2010-02-11 | 29.7 | 13.4 |\n",
    "| 2010-02-23 | 29.9 | 10.7 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0a1615",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Unrelated measurements in the same table\n",
    "Here you have a complex situation, but it is fairly easy to remedy. You probably want to create or choose an ID column, which relates to the item which you have the unrelated measurements, then create two (or more) tables that have an ID column and the relevantly grouped measurements. This is important when the number of repetitions due to the unrelated observables duplicate certain entries in different amounts, which could destroy the reliability of statistical processing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12496f48",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### One type of data in multiple tables\n",
    "This is very common, especially if every experiment writes to a file automatically, and you need to write and gather everything. The sequence to do this is quite simple:\n",
    "1. loop over the files and create a list of tables\n",
    "2. for each table, add a new column that records the filename (or some other unique identifier)\n",
    "3. combine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c730ff5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Easy to Implement Manipulation\n",
    "\n",
    "Once we have this we have the ability to do several things:\n",
    "- Filter and select\n",
    "    - Subsets or removal of values given a condition\n",
    "- Transformations\n",
    "    - Can apply functions taking one or more columns and appending / replacing a column with the result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd46bbd7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "- Aggregation\n",
    "    - collapsing multiple values into single values (statistical measures or any function you might want)\n",
    "- Sorting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7eaf68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "Daniel Bultrini"
   }
  ],
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "title": "Database Tutorial MOQS May 2022"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
