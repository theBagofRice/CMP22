{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computational modeling in python, SoSe2022\n",
    "\n",
    "Given are the following functions:\n",
    "\n",
    "\\begin{align}\n",
    "    f_1(x) &= x\\left(x-3\\right)\\left(x+3\\right) \\\\\n",
    "    f_2(x) &= \\left| x \\right| \\\\\n",
    "    f_3(x) &= \\sin \\left(2.1x\\right)\\left(-\\frac{x}{2}\\right) \\\\\n",
    "    f_4(x) &= 1.6^x -1.5x \\\\\n",
    "    f_5(x,y) &= \\sin\\left(x+y\\right)\\tan\\left(0.1x\\right) \\\\\n",
    "    f_6(x,y) &= \\sin\\left(\\sqrt{5}+x\\right)y \n",
    "\\end{align}\n",
    "\n",
    "\\- courtesy of Anna Bardroff \\- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "def function1(x):\n",
    "    y = x*(x - 3)*(x + 3)\n",
    "    return y\n",
    "\n",
    "def function2(x):\n",
    "    y = abs(x)\n",
    "    return y\n",
    "\n",
    "def function3(x):\n",
    "    y = sin(x * 2.1) * (-x / 2.0)\n",
    "    return y\n",
    "\n",
    "def function4(x):\n",
    "    y = 1.6**x - 1.5 * x\n",
    "    return y\n",
    "\n",
    "def function5(x,y):\n",
    "    z = sin(x + y) * tan(0.1 * x)\n",
    "    return z\n",
    "\n",
    "def function6(x,y):\n",
    "    z = sin(sqrt(5) + x) * y\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approaches to differentiate a function\n",
    "\n",
    "## Numerically from the definition of the derivative\n",
    "\n",
    "For numerical differentiation one can start with the definition of the derivative:\n",
    "\n",
    "\\begin{align}\n",
    "f'(x) \\approx \\frac{f(x+h)-f(x)}{h}\n",
    "\\end{align}\n",
    "\n",
    "with $h \\rightarrow 0$. The function $f$ needs to be evaluated at points with distance $h$. $h$, of course, cannot be arbitrarily small and needs to be of some finite value. This method is therefore also called the __finite difference__ method. \n",
    "\n",
    "\n",
    "\n",
    "### The numpy diff function:\n",
    "\n",
    "https://numpy.org/doc/stable/reference/generated/numpy.diff.html \n",
    "\n",
    "This function returns the difference between two consecutive elements of an array, as $f(x_{i+1})-f(x_i)$. Division by $dx$ will result in the above derivative. As diff() computes the differences, the returned array will have one fewer element than the original array for the first derivative. With diff also higher derivatives can be constructed (through recursive application of the difference) as shown below. \n",
    "<p>\n",
    "\n",
    "### The numpy gradient function\n",
    "    \n",
    "https://numpy.org/doc/stable/reference/generated/numpy.gradient.html\n",
    "    \n",
    "This function accepts an optional dx as an input (default: dx=1) and will compute the gradient using central differences \n",
    "    \n",
    "\\begin{align}\n",
    "    f'(x) \\approx \\frac12 \\left(\\frac{f(x+h)-f(x)}{h} + \\frac{f(x)-f(x-h)}{h}\\right) = \\frac{f(x+h)-f(x-h)}{2h}\n",
    "\\end{align}\n",
    "\n",
    "for the interior points and one-side differences at the boundaries and will thus return an array of the same shape as the original array. Higher derivatives must be constructed by applying gradient repeatedly.\n",
    "\n",
    "Limitations: round-off errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate in 21 points in interval [-4,4] \n",
    "a=-4\n",
    "b=4\n",
    "npoints = 21\n",
    "\n",
    "num,dx = linspace(a, b, npoints, retstep=True)\n",
    "\n",
    "f1 = function1(num)\n",
    "\n",
    "dydx_diff = diff(f1)/dx\n",
    "dydx_grad_e2 = gradient(f1, dx, edge_order=2)\n",
    "dydx_grad_e1 = gradient(f1, dx, edge_order=1)\n",
    "\n",
    "# put the diff points between original points\n",
    "num2 = num + dx/2 \n",
    "\n",
    "mf=18\n",
    "fig, ax = plt.subplots(figsize=(8,5))\n",
    "\n",
    "plt.plot(num, f1, label='f1')\n",
    "plt.plot(num2[0:-1], dydx_diff, label='diff', marker=\"o\")\n",
    "plt.plot(num, dydx_grad_e2, label='gradient,eorder=2', marker=\"x\")\n",
    "plt.plot(num, dydx_grad_e1, label='gradient,eorder=1')\n",
    "\n",
    "plt.xticks(fontsize=mf)\n",
    "plt.yticks(fontsize=mf)\n",
    "ax.set_xlabel('x value',fontsize=mf)\n",
    "ax.set_ylabel('y value',fontsize=mf)\n",
    "legend = ax.legend(loc='best', shadow=False,fontsize=mf,borderpad = 0.1,\n",
    "                   labelspacing = 0, handlelength = 0.8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#second derivative\n",
    "d2ydx2_diff = diff(f1,n=2)/dx**2\n",
    "d2ydx2_grad = gradient(dydx_grad_e2, dx, edge_order=2) # original input from e2\n",
    "\n",
    "# put the diff points again between original diff points\n",
    "num3 = num2 + dx/2\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,5))\n",
    "plt.plot(num,f1,label='f1')\n",
    "\n",
    "plt.plot(num2[0:-1],dydx_diff,label='diff')\n",
    "plt.plot(num,dydx_grad_e2,label='gradient')\n",
    "\n",
    "plt.plot(num3[0:-2],d2ydx2_diff,label='diff2', marker=\"o\")\n",
    "plt.plot(num,d2ydx2_grad,label='gradient2')\n",
    "\n",
    "plt.xticks(fontsize=mf)\n",
    "plt.yticks(fontsize=mf)\n",
    "ax.set_xlabel('x value',fontsize=mf)\n",
    "ax.set_ylabel('y value',fontsize=mf)\n",
    "\n",
    "legend = ax.legend(loc='best', shadow=False,fontsize=mf,\n",
    "                   borderpad = 0.1, labelspacing = 0,\n",
    "                   handlelength = 0.8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using autograd\n",
    "\n",
    "https://anaconda.org/conda-forge/autograd \\\n",
    "https://github.com/HIPS/autograd \n",
    "\n",
    "Open the anaconda terminal and type\n",
    "\n",
    "`conda install autograd -c conda-forge`\n",
    "\n",
    "\n",
    "What does it do? It relies on any complicated function being composed of simple continuous operations. The output of one operation is then interpreted as the input of the next operation.  Autograd tracks data through all simple operations using instrumented code ( similar to a debugger - this is why it needs its own numpy wrapper to be able to trace operations within numpy). From the record of operations it creates a call graph of simple operations. The derivative can then be evaluated with the chain rule.\n",
    "\n",
    "- Limitations: need to be able to track execution or elementary functions must be implemented with derivatives. Cannot handle certain operations like in-place assignments (x += y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autograd.numpy import *  # this is a numpy wrapper \n",
    "from autograd import grad\n",
    "\n",
    "dydx_autograd=[]\n",
    "\n",
    "# the grad function instruments the function passed to it and returns a function \n",
    "# that calculates the derivative. \n",
    "grad_fct = grad(function1)\n",
    "print(type(grad_fct))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grad can only handle scalar inpit, so we need to call it element wise\n",
    "for i1 in num:\n",
    "    dydx_autograd.append(grad_fct(i1))\n",
    "\n",
    "fnc = function1(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "fig, ax = plt.subplots(figsize=(8,5))\n",
    "plt.plot(num,fnc,label='f1')\n",
    "plt.plot(num,dydx_autograd,label='autograd')\n",
    "plt.xticks(fontsize=mf)\n",
    "plt.yticks(fontsize=mf)\n",
    "ax.set_xlabel('x value',fontsize=mf)\n",
    "ax.set_ylabel('y value',fontsize=mf)\n",
    "legend = ax.legend(loc='lower right', shadow=False,fontsize=mf,\n",
    "                   borderpad = 0.1, labelspacing = 0, handlelength = 0.8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `elementwise_grad` expects arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autograd import elementwise_grad as egrad \n",
    "\n",
    "# create the function and directly call it with argument 'num'\n",
    "dydx_autograd=egrad(function1)(num)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,5))\n",
    "plt.plot(num,f1,label='f1')\n",
    "plt.plot(num,dydx_autograd,label='autograd')\n",
    "plt.xticks(fontsize=mf)\n",
    "plt.yticks(fontsize=mf)\n",
    "ax.set_xlabel('x value',fontsize=mf)\n",
    "ax.set_ylabel('y value',fontsize=mf)\n",
    "legend = ax.legend(loc='lower right', shadow=False,fontsize=mf,borderpad = 0.1, labelspacing = 0, handlelength = 0.8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second derivative: just passt the derivative function again to autograd\n",
    "dydx_autograd=egrad(function1)(num)\n",
    "d2ydx2_autograd=egrad(egrad(function1))(num)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,5))\n",
    "plt.plot(num,f1,label='f1')\n",
    "plt.plot(num,dydx_autograd,label='autograd')\n",
    "plt.plot(num,d2ydx2_autograd,label='autograd2')\n",
    "plt.xticks(fontsize=mf)\n",
    "plt.yticks(fontsize=mf)\n",
    "ax.set_xlabel('x value',fontsize=mf)\n",
    "ax.set_ylabel('y value',fontsize=mf)\n",
    "legend = ax.legend(loc='lower right', shadow=False,fontsize=mf,borderpad = 0.1, labelspacing = 0, handlelength = 0.8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously a method of choice, especially for applications involving deep learning / neural networks where many derivatives are required (and is used in PyTorch, see for example - https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Symbolically using sympy\n",
    "\n",
    "Sympy is a python library for symbolic manipulations: https://www.sympy.org/en/index.html. \n",
    "\n",
    "I want to mention it here for completeness, but generally we will not perform any symbolic computations. But if you need symbolic manipulations and do not want to buy a super expensive Mathematica license, maybe sympy is a way to go.\n",
    "\n",
    "`conda install sympy`\n",
    "\n",
    "- Limitation: may create huge expressions for complicated functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sp\n",
    "\n",
    "x= sp.Symbol('x')\n",
    "\n",
    "function= x*(x-3)*(x+3)\n",
    "\n",
    "dydx_sp=sp.diff(function, x)\n",
    "print(\"Raw output:    \", dydx_sp)\n",
    "\n",
    "dydx_sp = sp.simplify(dydx_sp)\n",
    "print(\"After simplify:\", dydx_sp)\n",
    "\n",
    "#it comes with its own plotting routine\n",
    "p1 = sp.plot(function,(x,-4,4),label='f1',show=False)\n",
    "p2 = sp.plot(dydx_sp,(x,-4,4),label='sympy',show=False,line_color='orange')\n",
    "p1.append(p2[0])\n",
    "p1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partial derivatives\n",
    "\n",
    "The derivative of a function $f(x_1,x_2,\\ldots,x_n)$ of more than one variable with respect to one of those variables is given by the partial derivative of the function, $\\frac{\\partial f}{\\partial x_1} (x_1,x_2,\\ldots,x_n)$.\n",
    "\n",
    "The Jacobian matrix contains the first partial derivatives of a vectorized function $\\mathbf{f} (x_1,x_2,\\ldots,x_n)$,  $\\mathbf{f}: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m$, wrt all the dimensions:\n",
    "\\begin{align}\n",
    "\\mathbf{J} = \\left[ \\frac{\\partial \\mathbf{f}}{\\partial x_1}, \\frac{\\partial \\mathbf{f}}{\\partial x_2}, \\ldots, \\frac{\\partial \\mathbf{f}}{\\partial x_n} \\right] \n",
    "\\end{align}\n",
    "\n",
    "For example: \n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{f} \\left(\n",
    "\\begin{bmatrix}\n",
    "x \\\\\n",
    "y \n",
    "\\end{bmatrix}\n",
    "\\right)=\n",
    "\\begin{bmatrix} f_1(x,y) \\\\\n",
    "f_2(x,y) \\end{bmatrix} = \n",
    "\\begin{bmatrix} x^2+y  \\\\\n",
    "y+2y^2+x \\end{bmatrix} \n",
    "\\end{align}\n",
    "The Jacobian matrix of this function is\n",
    "\\begin{align}\n",
    "\\mathbf{J}(x,y) =\n",
    "\\begin{bmatrix} \\frac{\\partial f_1}{\\partial x} & \\frac{\\partial f_1}{\\partial y} \\\\\n",
    "\\frac{\\partial f_2}{\\partial x} & \\frac{\\partial f_2}{\\partial y} \\end{bmatrix} = \n",
    "\\begin{bmatrix} 2x & 1  \\\\\n",
    "1 & 4y + 1 \\end{bmatrix} \n",
    "\\end{align}\n",
    "and the Jacobian determinant is \n",
    "\\begin{align}\n",
    "\\det(\\mathbf{J}(x,y))  =\n",
    "(2x)\\cdot (4y+1) - 1 \\cdot 1 = 8xy + 2x - 2\n",
    "\\end{align}\n",
    "The Jacobian matrix maps a set of vectors onto another set of vectors, for example, the transformation from cartesian $x, y$ to polar $r, \\theta$ coordinates can be described through a Jacobian. __Because of this mapping, the Jacobian matrix is a tensor__. \n",
    "\n",
    "The Jacobian determinant is useful because it tells how the volume changes under the map.\n",
    "\n",
    "__This is an example of matrix calculus, which is fundamental to applications in quantum chemistry and machine learning__.\n",
    "\n",
    "Let's look at partial derivatives for an example function. We will use `autograd` for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is our sample function\n",
    "def myfunc(x,y):\n",
    "    z = x**2+y\n",
    "    return z\n",
    "\n",
    "#let's define a grid for evaluation and plotting so we can check the result\n",
    "a=-4\n",
    "b=4\n",
    "npoints = 100\n",
    "numx,deltax = linspace(a,b,npoints,retstep=True)\n",
    "numy,deltay = linspace(a,b,npoints,retstep=True)\n",
    "\n",
    "#this is the mesh for the plotting\n",
    "partialx = zeros((len(numx),len(numy))) # df/dx\n",
    "partialy = zeros((len(numx),len(numy))) # df/dy\n",
    "fin = zeros((len(numx),len(numy))) # f\n",
    "\n",
    "#derivative wrt first variable - x\n",
    "px = grad(myfunc,0)\n",
    "#derivative wrt second variable - y\n",
    "py = grad(myfunc,1)\n",
    "\n",
    "for j,x in enumerate(numx):\n",
    "    for k,y in enumerate(numy):\n",
    "        #value of the function at x,y\n",
    "        fin[j,k]=myfunc(x,y)\n",
    "        #value of partial derivative wrt x at x,y\n",
    "        partialx[j,k]=px(x,y)\n",
    "        #value of partial derivative wrt y at x,y\n",
    "        partialy[j,k]=py(x,y)\n",
    "            \n",
    "\n",
    "def plot2D(x,y,z,title):\n",
    "    mf = 16\n",
    "    Y, X = meshgrid(y,x)\n",
    "    fig = plt.figure(figsize=(10,8))\n",
    "    ax = plt.axes(projection='3d')\n",
    "    ax.plot_surface(X, Y, z,cmap = cm.viridis, rstride=1, cstride=1,edgecolor='none')\n",
    "    plt.xticks(fontsize=mf)\n",
    "    plt.yticks(fontsize=mf)\n",
    "    ax.zaxis.set_tick_params(labelsize=mf)\n",
    "    ax.set_xlabel('x value',fontsize=mf)\n",
    "    ax.set_ylabel('y value',fontsize=mf)\n",
    "    ax.set_zlabel('z value',fontsize=mf)\n",
    "    ax.set_title('{}'.format(title),fontsize=mf, weight='bold')\n",
    "    ax.view_init(30, 80)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def plot_im(x,y,z,title):\n",
    "    # x goes on the y axis\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8,5))\n",
    "    ax.imshow(z,extent=[x[0],x[-1],y[0],y[-1]])\n",
    "    ax.set_title('{}'.format(title),fontsize=mf, weight='bold')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot2D(numx,numy,fin,'myfunc')\n",
    "plot2D(numx,numy,partialx,'df/dx')\n",
    "plot2D(numx,numy,partialy,'df/dy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_im(numx,numy,fin,'myfunc')\n",
    "plot_im(numx,numy,partialx,'df/dx')\n",
    "plot_im(numx,numy,partialy,'df/dy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second derivatives: The Laplacian\n",
    "\n",
    "The Laplace operator generates the second derivatives of a multidimensional function, for example, in Hilbert space:\n",
    "\n",
    "\\begin{align}\n",
    "\\Delta f(x_1,x_2,\\ldots,x_n) = \n",
    "\\triangledown ^2  f(x_1,x_2,\\ldots,x_n) \n",
    "= \\sum_i^n \\frac{\\partial^2 f}{\\partial x_i^2}\n",
    "\\end{align}\n",
    "The Laplacian is extremely important in physics and appears for example in the Schr√∂dinger equation. Using the three-point finite differences method we can find an expression for the second derivative:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{d^2 f}{dx^2} \\approx \\frac{f(x_{j-1})-2f(x_j)+f(x_{j+1})}{h^2}\n",
    "\\end{align}\n",
    "\n",
    "Note that this expression has a very similar structure than in the interpolation problem. If $\\vec{f} = \\left\\{f(x_j)\\right\\}$ is a vector of function values at the points $x_j$ we can express model the action of the Laplacian as a matrix $\\mathcal{L}$ acting on a vector $\\vec{f}$:\n",
    "\n",
    "\\begin{align}\n",
    "\\vec{f''} =  \\mathcal{L} \\vec{f}\n",
    "\\end{align}\n",
    "\n",
    "with \n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal{L} = \\frac{1}{h^2}\\left[\n",
    "\\begin{array}{cccccc}\n",
    "-2 & 1 & & & & \\\\\n",
    "1 & -2 & 1 & & & \\\\\n",
    "& \\ddots & \\ddots & \\ddots & & \\\\\n",
    "& & 1 & -2 & 1 & \\\\\n",
    "& & & \\ddots & \\ddots & \\ddots \\\\\n",
    "& & & & 1 & -2 \\\\\n",
    "\\end{array} \n",
    "\\right]\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "\n",
    "For a numerical representation of the Laplacian, the Laplacian matrix is introduced:\n",
    "\\begin{align}\n",
    "\\mathcal{L} = \\mathbf{D}-\\mathbf{A} \n",
    "\\end{align}\n",
    "where $\\mathbf{D}$ is the degree matrix (a diagonal matrix containing the degree (=number of connections/neighbours) of each vertex $v_i$) and $\\mathbf{A}$ is the adjacency matrix (which is 0 everywhere and 1 for adjacent vertices $v_i$ and $v_j$, that is vertices that are connected). As such, the Laplacian matrix contains the degree on the diagonal and ones on the off-diagonal for adjacent vertices.\n",
    "\n",
    "\n",
    "The second derivative operator will appear on the diagonal on the Laplacian matrix and one on the off-diagonal for adjacent grid points.\n",
    "\n",
    "Side-remark: The matrix vector multiplication will return wrong results for the end points. This one needs to keep in mind or fix, for instance by adding extra points to the left and right.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=-4\n",
    "b=4\n",
    "nsteps=9\n",
    "myvals,h=linspace(a,b,nsteps,retstep=True)\n",
    "Laplacian=(-2.0*diag(ones(nsteps))+diag(ones(nsteps-1),1)+diag(ones(nsteps-1),-1))/h**2\n",
    "print('Laplacian:\\n',Laplacian)\n",
    "print('Diagonal matrix:\\n',diag(ones(nsteps)))\n",
    "print('-Adjacency matrix part 1:\\n',diag(ones(nsteps-1),1))\n",
    "print('-Adjacency matrix part 2:\\n',diag(ones(nsteps-1),-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply this to a sample function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some function\n",
    "def myfunc2(x):\n",
    "    y = 2*x**3\n",
    "    return y\n",
    "\n",
    "# exact derivative of the function\n",
    "def d2myfunc2(x):\n",
    "    y = 3*2*2*x**1\n",
    "    return y\n",
    "\n",
    "a=-1\n",
    "b=1\n",
    "nsteps=101\n",
    "\n",
    "myvals,h=linspace(a,b,nsteps,retstep=True)\n",
    "Laplacian=(-2.0*diag(ones(nsteps))+diag(ones(nsteps-1),1)+diag(ones(nsteps-1),-1))/(h**2)\n",
    "\n",
    "inp = myfunc2(myvals)\n",
    "outp = Laplacian @ inp # dot(Laplacian,inp)\n",
    "\n",
    "print(outp)\n",
    "\n",
    "#delete the endpoints, because we would need a special formula for them and do not want to bother at this point\n",
    "outp = delete(outp,0)\n",
    "outp = delete(outp,-1)\n",
    "\n",
    "print(outp)\n",
    "\n",
    "# compare with exact analytical derivative\n",
    "d2exact = d2myfunc2(myvals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(myvals,inp,label='f(x)')\n",
    "plt.plot(myvals[1:-1],outp,marker='x',markevery=5,label=r'd$^2$ f(dx$^2$), L')\n",
    "plt.plot(myvals,d2exact,linestyle = ':',label=r'd$^2$ f(dx$^2$),exact',markevery=10, marker=\"o\")\n",
    "legend = plt.legend(loc='upper left', shadow=False,fontsize=16,borderpad = 0.1, labelspacing = 0, handlelength = 0.8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "\n",
    "Compute and plot the first and second derivative of the above one-dimensional functions using your method of choice in the value range from -4 to 4 using an appropriate grid spacing.\n",
    "\n",
    "# Task 2\n",
    "\n",
    "Compute and plot the partial first derivatives of the above two-dimensional functions using autograd in the value range from -4 to 4 using an appropriate grid spacing. \n",
    "\n",
    "# Task 3\n",
    "Construct and plot the second derivative of function1, $d^2f_1/dx^2$, through the Laplacian matrix in the value range from -4 to 4 using an appropriate grid spacing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
